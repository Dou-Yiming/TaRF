<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" href="main.css">
  <!-- <link rel="icon" type="image/x-icon" href="assets/favicon.png"> -->
  <title>TaRF</title>
</head>

<body>
  <div id="video_grid">
    <div class="video_wrapper">
      <div class="video_container">
        <video autoplay muted playsinline loop>
          <source src="assets/grid/table_indoor.mp4" type="video/mp4">
        </video>
        <div class="overlay">Office</div>
      </div>
    </div>
    <div class="video_wrapper">
      <div class="video_container">
        <video autoplay muted playsinline loop>
          <source src="assets/grid/bench.mp4" type="video/mp4">
        </video>
        <div class="overlay">Bench</div>
      </div>
    </div>
  </div>

  <div id="title_slide">
    <div class="title_left">
      <h1>Tactile-Augmented Radiance Fields</h1>
      <div class="author-container-1">
        <div class="author-item"><a href="https://dou-yiming.github.io/">Yiming Dou<sup>1</sup></a></div>
        <div class="author-item"><a href="https://fredfyyang.github.io/">Fengyu Yang<sup>2</sup></a></div>
        <div class="author-item"><a href="">Yi Liu<sup>1</sup></a></div>
        <div class="author-item"><a href="https://antonilo.github.io/">Antonio Loquercio<sup>3</sup></a></div>
        <div class="last-author-item"><a href="https://andrewowens.com/">Andrew Owens<sup>1</sup></a></div>
      </div>
      <div class="mobile-author-container-1">
        <div class="author-item"><a href="https://dou-yiming.github.io/">Yiming Dou</a></div>
        <div class="author-item"><a href="https://fredfyyang.github.io/">Fengyu Yang</a></div>
        <div class="author-item"><a href="">Yi Liu</a></div>
        <div class="author-item"><a href="https://antonilo.github.io/">Antonio Loquercio</a></div>
        <div class="last-author-item"><a href="https://andrewowens.com/">Andrew Owens</a></div>
      </div>
      <div class="author-container-1">
        <div class="affiliation-item"><sup>1</sup>University of Michigan</div>
        <div class="affiliation-item"><sup>2</sup>Yale University</div>
        <div class="last-affiliation-item"><sup>3</sup>University of California, Berkeley</div>
      </div>
      <div class="author-container-1">
        <div class="last-affiliation-item">CVPR 2024</div>
      </div>
      <div class="button-container">
        <a href="" class="button">arXiv</a>
        <!-- <a href="" class="button">Video</a> -->
        <a href="https://github.com/Dou-Yiming/TaRF" class="button">Code</a>
        <a href="https://github.com/Dou-Yiming/TaRF" class="button">Data</a>
      </div>
    </div>
  </div>

  <div id="video_grid">
    <div class="video_wrapper">
      <div class="video_container">
        <video autoplay muted playsinline loop>
          <source src="assets/grid/table_outdoor.mp4" type="video/mp4">
        </video>
        <div class="overlay">Table</div>
      </div>
    </div>
    <div class="video_wrapper">
      <div class="video_container">
        <video autoplay muted playsinline loop>
          <source src="assets/grid/lounge.mp4" type="video/mp4">
        </video>
        <div class="overlay">Lounge</div>
      </div>
    </div>

  </div>

  <div id="title_slide">
    <div class="title_middle">
      <h1>Abstract</h1>
      <div id="abstract" class="grid-item">
        <p>
          We present a scene representation, which we call a tactile-augmented radiance field (TaRF), that brings vision
          and touch into a shared 3D space.
          This representation can be used to estimate the visual and tactile signals for a given 3D position within a
          scene.
          We capture a scene's TaRF from a collection of photos and sparsely sampled touch probes.
          Our approach makes use of two insights: (i) common vision-based touch sensors are built on ordinary cameras
          and thus can be registered to images using methods from multi-view geometry, and (ii) visually and
          structurally similar regions of a scene share the same tactile features.
          We use these insights to register touch signals to a captured visual scene, and to train a conditional
          diffusion model that, provided with an RGB-D image rendered from a neural radiance field, generates its
          corresponding tactile signal.
          To evaluate our approach, we collect a dataset of TaRFs. This dataset contains more touch samples than
          previous real-world datasets, and it provides spatially aligned visual signals for each captured touch signal.
          We demonstrate the accuracy of our cross-modal generative model and the utility of the captured visual-tactile
          data on several downstream tasks.
        </p>
      </div>
    </div>
  </div>


  <hr class="rounded">
  <div id="overview">
    <h1>Capturing Vision and Touch Signals</h1>
    <h2>Capturing Setup</h2>
    <p>
      We mount an RGB-D camera to one end of a rod, and a tactile sensor to the other end.
      Next, record temporally-aligned vision-touch pairs.
      The relative pose between the camera and the tactile sensor
      is solved by using visual-tactile correspondences.
      Since the camera and the tactile sensor are approximately orthogonal,
      we first synthesize a novel view by rotating the camera 90&deg; on the x-axis and then
      manually annotate the correspondences between the touch measurement and synthesized frame.
    </p>
    <div class="barplot">
      <img src="assets/figs/setup.jpg" width="40%">
    </div>
    <p>
      The calibration procedure of estimating the 6DoF relative pose
      \( ({\mathbf R}, {\mathbf t}) \)
      between the camera and the tactile sensor
      is formulated as a resectioning problem.
      <!-- <br> -->
      We obtain 3D points \( \{\mathbf{x}_i\}_{i=1}^M \) for each annotated correspondences.
      Each point has a pixel position \( \mathbf{u}_i \in \mathcal{R}^2\) in the touch measurement.
      We find \( ({\mathbf R}, {\mathbf t}) \) by minimizing the reprojection error:
    </p>

    <p style="text-align: center;">
      \( \min_{{\mathbf R}, {\mathbf t}} \frac{1}{M}\sum_{i=1}^M \lVert \pi({\mathbf
      K}[\mathbf{R}\,\,|\,\,\mathbf{t}], \mathbf{X}_i) - \mathbf{u}_i \rVert_1, \)
    </p>
    <p>
      where \( \pi \) is the perspective projection matrix,
      \( \mathbf{K} \) is the intrinsics of the tactile sensor's camera, and
      the point \( \mathbf{X}_i \) is in the coordinate system of the generated vision frame.
    </p>



    <h2>A 3D Visual-Tactile Dataset</h2>
    <p>
      First, we collect multiple views from the scene around the areas we plan to touch.
      Next, we collect synchronized visual and touch data.
    </p>
    <div class="barplot">
      <video src="assets/figs/capture.mp4" width="25%" autoplay muted playsinline loop></video>
    </div>
    <p>
      We then estimate the camera location of the vision frames collected in the previous two stages using SfM.
      After estimating the camera poses for the vision frames,
      the touch measurements' poses can be derived by using the relative pose.
      Finally, we associate each touch sensor with a color image by translating the sensor poses upwards and querying
      the NeRF with such poses. Note that the white box on the center of the visual image corresponds to the touched
      area.
    </p>
    <div class="barplot">
      <div class="image_container">
        <img src="assets/figs/dataset.jpg">
      </div>
    </div>
    <p>
      Compared to previous works, our dataset is the first to capture
      quasi-dense, scene-level, and spatially-aligned visual-tactile data.
      We call the resulting scene representation a tactile-augmented radiance field (TaRF).
    </p>
    <div class="barplot">
      <img src="assets/figs/dataset_comp.jpg" width="30%">
    </div>


    <h1>Imputing Missing Touch</h1>
    <h2>Method Overview</h2>
    <p>
      With the captured dataset, we train a diffusion model to estimate touch
      at locations not directly probed by a sensor.
      The model is conditioned on the egocentric visual images and depth maps extracted from the NeRF.
      Through the touch estimation, the model effectively propagates sparse touch samples,
      obtained by probing, to other visually/structurally similar regions of the scene.
    </p>
    <div class="barplot">
      <img src="assets/figs/method.jpg" width="35%">
    </div>

    <h2>Dense Touch Estimation</h2>
    <p>
      Queried with arbitrary 3D positions in the scene,
      our model generates touch samples with many details in micro-geometry of various materials.
    </p>
    <div class="barplot">
      <video src="assets/grid/table_indoor.mp4" width="22%" autoplay muted playsinline loop></video>
      <video src="assets/grid/table_outdoor.mp4" width="22%" autoplay muted playsinline loop></video>
    </div>

    <h1>Downtream Tasks</h1>
    <h2>Tactile Localization</h2>
    <p>
      Given a tactile signal and a visual image, we ask the question:
      <i>what part of this image feel like this?</i>
      To do this, we first train a contrastive model that maps the visual and tactile signals
      into a shared embedding space.
      Next, we split the visual image into patches and compute the similarity between each patch and the tactile signal,
      which is plotted as a heatmap.
      The heatmaps show that our model successfully cpatures the correlations between the visual and tactile data.
    </p>
    <div class="barplot">
      <div class="image_container">
        <img src="assets/figs/heatmap.jpg">
        <div class="caption" style="margin-top: 0.0vw">
        </div>
      </div>
    </div>

    <h2>Material Classification</h2>
    <p>
      We investigate the efficacy of our TaRF representation for understanding material properties.
      We follow the formulation by
      <a href="https://touch-and-go.github.io/">Yang <i>et al. </i></a>,
      which consists of three subtasks: (i) material classification,
      (ii) softness classification and (iii) roughness classification.
      Combining our dataset with the original data improves the effectiveness of pretraining,
      and adding estimated touch signals further achieves high performance on all the three tasks.
      This indicates that not only does our dataset covers various materials
      but also the diffusion model captures distinguishable and useful patterns.
    </p>
    <div class="barplot">
      <img src="assets/figs/classification_results.jpg" width="40%">
    </div>

    <h2>Related Works</h2>
    <p>
      This project is highly related to the previous work
      <a href="https://proceedings.mlr.press/v205/zhong23a.html"><i>Touching a NeRF</i></a>,
      which also estimates touch from NeRF representation using object-centric data collected from robots.
    </p>


    <h3>BibTeX</h3>
    <p class="bibtex">

    </p>
  </div>
  <script type="text/javascript">
    /* https://stackoverflow.com/questions/3027707/how-to-change-the-playing-speed-of-videos-in-html5 */
    document.querySelector('video').defaultPlaybackRate = 1.0;
    document.querySelector('video').play();

    var videos = document.querySelectorAll('video');
    for (var i = 0; i < 1; i++) {
      videos[i].playbackRate = 1.0;
    }
  </script>
  <script>
    /* https://stackoverflow.com/questions/21163756/html5-and-javascript-to-play-videos-only-when-visible */
    var videos = document.getElementsByTagName("video");

    function checkScroll() {
      var fraction = 0.5; // Play when 70% of the player is visible.

      for (var i = 0; i < 1; i++) {  // only apply to the first video

        var video = videos[i];

        var x = video.offsetLeft, y = video.offsetTop, w = video.offsetWidth, h = video.offsetHeight, r = x + w, //right
          b = y + h, //bottom
          visibleX, visibleY, visible;

        visibleX = Math.max(0, Math.min(w, window.pageXOffset + window.innerWidth - x, r - window.pageXOffset));
        visibleY = Math.max(0, Math.min(h, window.pageYOffset + window.innerHeight - y, b - window.pageYOffset));

        visible = visibleX * visibleY / (w * h);

        if (visible > fraction) {
          video.play();
        } else {
          video.pause();
        }

      }

    }
    window.addEventListener('scroll', checkScroll, false);
    window.addEventListener('resize', checkScroll, false);
  </script>
  <script>
    // Function to check if the user is on a mobile device
    function isMobileDevice() {
      return /Mobi|Android/i.test(navigator.userAgent);
    }

    // If the user is on a mobile device, disable autoplay
    if (isMobileDevice()) {
      const videos = document.querySelectorAll('video');
      videos.forEach(video => {
        video.autoplay = false;
      });
    }
  </script>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=51e0d73d83d06baa7a00000f"
    type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous">
    </script>
  <script src="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/js/webflow.fd002feec.js"
    type="text/javascript"></script>
  <table>
    <tr style="padding:300px" hidden>
      <script type="text/javascript" id="clustrmaps"
        src="//clustrmaps.com/map_v2.js?d=OtDL9EAoszk_Q1mc0iANVlJ5QaSt_aX952dIn--u4Zk&cl=ffffff&w=a">
        </script>
    </tr>
  </table>

</body>

</html>